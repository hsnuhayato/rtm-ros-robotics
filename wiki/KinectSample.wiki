#summary Kinectのサンプル
<wiki:toc max_depth="2" />

= 概要 =

ここで紹介するKinectのサンプルは，
現時点では、
 * jsk-ros-pkgのroseus_tutorialsにあるkinect.launch 
 * jsk-ros-pkgのroseus_tutorialsにあるkinect-model-viewer.l
を紹介します。

kinectの主なサンプルプログラム(launch)は、jsk-ros-pkg以下の
 * roseus_tutorialsのkinect.launch

jsk-ros-pkg以下のcppのサンプルは、
 * jsk-openni-kinect/openni_pointer/src/openni_pointer.cpp
 * jsk-openni-kinect/openni_scene/src/openni_scene.cpp
 * jsk-openni-kinect/openni_swipe/src/openni_swipe.cpp
/opt/ros/fuerte/stacks以下のcppのサンプルは、
 * openni-tracker/src/openni_tracker.cpp

jsk-ros-pkg以下のeusのサンプルは
 * roseus_tutorials/kinect-model-view.l
 * roseus_tutorials/openni-swipe.l
 * roseus_tutorials/openni-pointer.l

などがあると思いますので、適宜参照してみてください。

= 準備 =
 * [ROS_Install]にしたがってROSと講義関連パッケージがインストールされていることが前提です．
 * Kinectを使います。特に必要な設定はありません。Kinect For Windowsでは動作がどうなるか確認できておりません。
 * OSのバージョンによってはうまくいかないものもあります。
 * 仮想環境からKinectの情報を取得するのも難しいようですので、注意してください。
=kinect.launch=
==kinect.launchの実行 ==
２つのターミナルを立ち上げます
まずは，1つ目で
{{{
roscore
}}}
として下さい．

次に2つ目のターミナルで
{{{
roslaunch roseus_tutorials  kinect.launch
}}}
とします．

そうすると、小さな画面が表示され、デプスの値によって色が分けて表示されます。(Kinectの近くに手をざすとうまく動かないので、50cmほどkinectから離してかざしてみてください)





[http://rtm-ros-robotics.googlecode.com/svn/wiki/kinect_launch_imageView.png]


プログラムがうまくいく、手だけではなく全身を写す（上半身だけでも可）と、
実行した二つ目のターミナルに
{{{
[ INFO] [1352997345.849554969]: New User 1
[ INFO] [1352997349.105289471]: Pose Psi detected for user 1
[ INFO] [1352997349.571060727]: Calibration started for user 1
[ INFO] [1352997350.071415612]: Calibration failed for user 1
[ INFO] [1352997350.071990197]: Pose Psi detected for user 1
[ INFO] [1352997364.279894287]: Lost user 1
}}}
などの表示が見られると思います。（人によって表示される順番や事柄は変わります。）


次にrostopicはどのようなものがあるかを確認します。
以下のコマンドを新しいターミナルで実行してください。
{{{
rostopic list
}}}
を行ってください。
このサンプル以外のものを行っていなければ、大量に表示されるtopicの最後に/tfがあるのが確認できると思います。さらに、
{{{
rostopic echo /tf
}}}
を行うことでtfのtopicに今流れてきている値を確認できると思います。

一応確認するために、/tfのmsgの形は、
{{{
rostopic type /tf
}}}
で、/tfのメッセージのタイプがわかりその内容は
{{{
rosmsg show tf/tfMessage
}}}
で見れます。
{{{
geometry_msgs/TransformStamped[] transforms
  std_msgs/Header header
    uint32 seq
    time stamp
    string frame_id
  string child_frame_id
  geometry_msgs/Transform transform
    geometry_msgs/Vector3 translation
      float64 x
      float64 y
      float64 z
    geometry_msgs/Quaternion rotation
      float64 x
      float64 y
      float64 z
      float64 w
}}}
のようになります。

ここで、roslaunch roseus_tutorials  kinect.launch
を行ったターミナルを開きながら、カメラの前で肘を直角にして両腕をあげてジッとしてみてください。





[http://rtm-ros-robotics.googlecode.com/svn/wiki/kinect_sample_hangup_pose.png]

{{{
[ INFO] [1352998662.710511745]: Calibration started for user 3
[ INFO] [1352998664.822129074]: Calibration complete, start tracking user 3
}}}
などが表示されると思います。そうすると、rostopic echo /tfを行ったターミナルを開いているならば、/tfの値も変更されて、ターミナルに体の各部位の情報が表示される様になります。

ここで、tfでの値を3Dで表示しましょう。
また新しいターミナルで以下のコマンドを実行してください。
{{{
rosrun rviz rviz
}}}
起動後に、左側のコラムのなかにaddのボタンがあると思います。

以下のものを開いたウィンドウでaddしていってください。
 * rviz/Axes
 * rviz/Grid
 * rviz/Camera
 * rviz/TF
それぞれ、基準の座標、グリッド、カメラ、体の部位の表示を
表してくれます。

しっかりと、トラックが行われている状態であれば、
体の表示がrvizの中に見れると思います。



[http://rtm-ros-robotics.googlecode.com/svn/wiki/kinect_sample_rviz_view.png]

==kinect.launchのソース・概略==
roseus_tutorials/roskinect.launchの中を軽く見てます。
{{{
 <node name="openni_tracker" pkg="openni_tracker" type="openni_tracker" output="screen" respawn="true" >
    <param name="camera_frame_id" value="camera_depth_frame"/>
  </node>
  <node name="openni_dispairty_view" pkg="image_view" type="disparity_view">
    <remap from="image" to="/camera/depth/disparity" />
  </node>
  <node name="map_openni_camera" pkg="tf" type="static_transform_publisher"
        args="0 0 1.5 0 0 0 /map /camera_link 100" />
}}}
となっているのが分かると思います。

上から、nodeとして、pkg=""あたりで、openni_trackerと、image_viewと、tfに該当するものが、作られているのがわかります。openni_trackerは、roscd openni_trackerで分かると思いますが、/opt/ros/fuerte/stacksのなかにパッケージがあり、これは、ユーザーを検知に関するnodeです。image_viewは、depthの結果を表示するnodeです。tfは体の関節などの情報が送られるnodeです。

何となく、どのようなnodeを呼び出しているかは簡単には分かったと思います。
他にどのようなnodeが立ち上がるかを確認するには、新しいターミナルで
{{{
rosnode list
}}}
を実行します。先ほど上にかかれたopenni_tracker openni_disparity_view map_openni_cameraを含めいくらかのnodeが確認できると思います。

tfについての詳しいことは、ROSwikiの[http://www.ros.org/wiki/tf/Tutorials/Introduction%20to%20tf tf]を参照してください。



=kinect-model-viewer.l=
このサンプルは、kinect.launchで作られたtfの体各位の情報をとってきて、IRT Viewerで表示させるもので、上のrvizのような表示が見れます。
== kinect-model-viewer.lの実行 ==
kinect.launchも実行する必要があるので、まず、それらを準備します。

{{{
roscore
}}}
{{{
roslaunch roseus_tutorials kinect.launch
}}}

これでtfのtopicに必要な情報が送られます。

そしたら次に、kinect-model-viewer.lを実行します。
{{{
rosrun roseus_tutorials kinect-model-viewer.l "(test-kinect)"
}}}
としてください。
これは、kinect-model-viewer.lをroseusを立ち上げて、(test-kinect)をその中で実行しています。
もし以下のエラーが出る人は、
{{{
roseus : command not found
}}}
[http://code.google.com/p/rtm-ros-robotics/wiki/ROS_Example_TroubleShooting#roseus_:_command_not_found_と言われる． 困ったときは]
を参考に.bashrcを更新してもう一度試して見てください。

実行したら、IRT Viewerに棒人間が表示されると思います。

[http://rtm-ros-robotics.googlecode.com/svn/wiki/kinect_sample_model_view.png]


==kinect-model-viewer.lのソース・概略==
kinect-model-viewer.lの中身を簡単に見ていきます。

かなりコードは長いですが、コードの最後の方の一部のみを見ていきたいと思います。

{{{
#-:ros
(unless (find-package "ROS") (make-package "ROS"))
#+:ros
(defun test-kinect (&key robot fname loop-hook) ;; :fname "test-kinect.bvh", loop-hook is a function to call inside do-until-key loop
  (let ((floor (make-cube 2000 1000 10 :pos #f(1000 0 0))) f)
    (if fname (setq f (open fname :direction :output)))
    (ros::roseus "kinect_bvh")
    (if (not (boundp '*tl*))
        (setq *tl* (instance ros::transform-listener :init)))
    (setq b (make-kinect-bvh-robot-model))
    (objects (list floor b))
    (if robot (objects robot))
    ;;(defun find-node (name) (find name *arrows* :key #'(lambda (x) (send x :name)) :test #'string=))
    (if f (send b :dump-hierarchy f))
    (do-until-key
     (let ()
       (when (send b :copy-state-from-tf *tl*)
	 (format t "kinect robot : larm pos=~A, rarm pos=~A (world)~%"
		 (send b :larm :end-coords :worldpos)
		 (send b :rarm :end-coords :worldpos))
	 (format t "             : larm pos=~A, rarm pos=~A (local)~%"
		 (send (send (send b :torso :end-coords) :transformation (send b :larm :end-coords)) :worldpos)
		 (send (send (send b :torso :end-coords) :transformation (send b :rarm :end-coords)) :worldpos)))
       (when robot
	 (send b :copy-state-to robot))
       (if (functionp loop-hook) (funcall loop-hook))
       (send (get *viewer* :pickviewer) :look-all)
       (x::window-main-one)
       (if f (send b :dump-motion f))
       )) ;; do-until-key
    ))

;; (test-kinect)
}}}

ソースの上半分は、この上のコードで使用するためのクラスの定義や、tfからの情報の取得や反映をしてくれるものを実装してあり、このtest-kinectはそれらの関数を用いたプログラムを書いてます。

一行ずつ見ていきます。

{{{
#-:ros
(unless (find-package "ROS") (make-package "ROS"))
#+:ros
}}}
この#-:rosは「rosがないならば次の（で始まるものを実行する」という意味で、#+:rosはその反対に「rosがあるならば次の（で始まるものを実行する」という意味になります。

{{{
(if fname (setq f (open fname :direction :output)))
    (ros::roseus "kinect_bvh")
    (if (not (boundp '*tl*))
        (setq *tl* (instance ros::transform-listener :init)))
    (setq b (make-kinect-bvh-robot-model))
    (objects (list floor b))
}}}

ここでは、bの棒人間の準備をしたり、必要なものを準備してます。読み飛ばして構いませんが、ここで、IRT Viewerの表示するものも用意していることだけは知っておいてください。

{{{
    (do-until-key
     (let ()
}}}
ここから、ループが始まります。

{{{
       (when (send b :copy-state-from-tf *tl*)
	 (format t "kinect robot : larm pos=~A, rarm pos=~A (world)~%"
		 (send b :larm :end-coords :worldpos)
		 (send b :rarm :end-coords :worldpos))
	 (format t "             : larm pos=~A, rarm pos=~A (local)~%"
		 (send (send (send b :torso :end-coords) :transformation (send b :larm :end-coords)) :worldpos)
		 (send (send (send b :torso :end-coords) :transformation (send b :rarm :end-coords)) :worldpos))
}}}
(send b :copy-state-from-tf *tl*)においては、「人のトラックができているなら」で条件になってます。実際この関数では、kinectのトラッカーでの情報をbのロボットに対して反映させているものです。トラックができているときに、その後のformat以下の命令が実行されます。

{{{
(send b :larm :end-coords :worldpos)
}}}
で人間の左手の端点のカメラ原点の座標を取得することができます。
ここでの流れは、

kinect -> tf -> b（ロボットのモデル）->取得

というようになっています。それをformatで表示させています。
{{{
(send (send (send b :torso :end-coords) :transformation (send b :larm :end-coords)) :worldpos)
}}}
では、(send b :torso :end-coords)では、腰の座標系を取得、(send b :larm :end-coords)はさきほどと同じように、左手の端点の座標系を取得。

(send (send b :torso :end-coords) :transformation (send b :larm :end-coords))でそれらの相対的な位置関係を取得しています。ですので、絶対座標系の原点の位置にはよりません。

その返り値に対して、send  :worldposをすることで、coordsから３次元のベクトルのみを取り出してます。

{{{
(when robot
	 (send b :copy-state-to robot))
       (if (functionp loop-hook) (funcall loop-hook))
       (send (get *viewer* :pickviewer) :look-all)
       (x::window-main-one)
       (if f (send b :dump-motion f))
       ))
}}}
その後の部分では、画面の更新や、情報の更新を行っております。
===pr2も表示したい===
IRT Viewerに棒人間モデルの他にpr2を表示したいときは
先ほどの
{{{
#-:ros
(unless (find-package "ROS") (make-package "ROS"))
#+:ros
}}}
の部分を以下で上書きしてください。
{{{
#-:ros
(unless (find-package "ROS") (make-package "ROS"))
#+:ros
(progn
  (load "package://pr2eus/pr2-utils.l")
  (load "package://pr2eus/pr2-interface.l")
  )
#+:ros
}}}
さらに、
以下のコードを(do-until-keyの手前に追加してください。

{{{
    (pr2)
    (if (not (boundp '*irtviewer*)) (make-irtviewer))
    (objects (list *pr2* floor b))
    (send *irtviewer* :draw-objects)
}}}
これで、先ほどのIRTViewerにpr2のモデルも表示されると思います。

また、実機へ角度を送ったりするには、いつも通り上記のコードの後に
{{{
(setq *ri* (instance pr2-interface :init))
}}}
を追加すれば、:angle-vectorなどで動かすことができます。


==実機でやるときは==

実機のpr2でコードを動かす際は以下の事に注意してください。
 * kinect-model-viewer.lの中の、座標系が、デフォルトでmapになっており、kinectでテストしているときは、kinect真下を座標の原点にしてますが、pr2でのmapはpr2の足元が原点とはなりません。/mapとなっているところはすべて/base_footprintに変えてください。
 * kinect.launchをpr2の方で起動する必要がありますので、kinect.launchがpr2で起動していることを確認してください。

=Kinectからの画像をとりあえず見る。=
Kinectから得られる画像をとりあえず見るだけです。
簡単に、Kinectからの様々な画像がくるものをtopicにpublishしてもらうには、
{{{
roslaunch openni_launch openni.launch
}}}
とすればよいです。
そうすると、さまざまなtopicが以下のように打つことで立つことが確認できます。
{{{
rostopic list
}}}
代表的なところでは
{{{
/camera/rgb/image_rect_color
/camera/rgb/image_rect
}}}
などが立ちあがっております。
以下では、二通りで、image_view2で表示したいと思います。
==rosrunからimage_view2==
この場合、image_view2をさきほどのopenni_launchとは別のターミナルにimage_view2を立ち上げたいと思います。
{{{
rosrun image_view2 image_view2 image:=/camera/rgb/image_rect
}}}
としてください。
そうすると、/camera/rgb/image_rectに流れてきているkinectからの画像を持ってきて、image_view2のウィンドウに表示します。
{{{
image:=/camera/rgb/image_rect
}}}
の部分ですが、[http://code.google.com/p/rtm-ros-robotics/wiki/ROS_Example_ImageProcessing 画像処理のサンプル]でもあるように、リマップという機能です。
image_view2の中のコードでimageからとってこいという命令を/camera/rgb/image_rectからとってこいに変えてしまう機能という程度の理解で大丈夫だと思います。この部分を他の/camera/rgb/image_rect_colorなどに変え、メッセージの型があえば、表示されると思います。
==launchファイルから立ち上げる==
launchファイルの使い方も含めて、立ち上げかたを書いてみようと思います。
先ほど、立ち上げたopenni_launchは閉じていただいて大丈夫です。


もし、roslaunchのタグや、文法について詳しく知りたい場合は、[http://www.ros.org/wiki/roslaunch/XML ROSWikiのroslaunch/XML]を参照してください。
以下の内容のファイルを例えばjsk-ros-pkg/roseus_tutorials/launch（どこでもよいです。）などに保存してください。ここでは,kinectImage.launchとでもしておきます。

{{{
<launch>
  //以下のincludeタグはopenni.launchを立ち上げます。
  <include file="$(find openni_launch)/launch/openni.launch">
    <arg name="respawn" value="true"/>
    <arg name="publish_tf" value="true"/>
  </include>

  //以下のnodeタグはimage_view2のnodeを立ち上げます。
  <node name="image_view2" pkg="image_view2" type="image_view2" output="screen" >
    <!--remap from="image" to="/camera/rgb/image_rect" /-->
    <remap from="image" to="/camera/rgb/image_rect_color/" />
    <param name="autosize" value="true" />
    <param name="blurry" value="false" />
    <param name="window_name" value="image_view2">
  </node>
</launch>
}}}
以上です。
===実行===
これをとりあえず、実行してみます。
{{{
roslaunch roseus_tutorials kinectImage.launch
}}}
おそらく、カラー画像がimage_viewに見えていると思います。

===コード===
初めのincludeの中から見ていきます。
{{{
  <include file="$(find openni_launch)/launch/openni.launch">
    <arg name="respawn" value="true"/>
    <arg name="publish_tf" value="true"/>
  </include>
}}}
ですが、これは、先ほどのコメントにもあるように、openni.launchを実行して、kinectからの画像を得られるtopicを立ててもらってます。
{{{
$(find openni_launch)/launch/openni.launch
}}}
どこかにある"openni_launch"のパッケージを見つけだし、そのなかのlaunchフォルダのopenni.launchを実行させてます。includeタグは、既に作られている他のlaunchフォルダを含めるためのもので、launchファイルを再利用するのに便利です。
{{{
    <arg name="respawn" value="true"/>
    <arg name="publish_tf" value="true"/>
}}}
includeタグの中のargは、openni.launchの中の変数をvalueに設定しなおしてます。
おそらく何をしているかわからないと思うので、以下がopenni.launchの中の一部を持ってきました。
{{{
<launch>

  <!-- "camera" should uniquely identify the device. All topics are pushed down
       into the "camera" namespace, and it is prepended to tf frame ids. -->
  <arg name="camera" default="camera" />
...
  <arg name="publish_tf" default="true" />

  <!-- Disable bond topics by default -->
  <arg name="bond" default="false" /> <!-- DEPRECATED, use respawn arg instead -->
  <arg name="respawn" default="$(arg bond)" />

  <!-- Start nodelet manager in top-level namespace -->
  <arg name="manager" value="$(arg camera)_nodelet_manager" />
...
...
  <!-- Load reasonable defaults for the relative pose between cameras -->
  <include if="$(arg publish_tf)"
	   file="$(find openni_launch)/launch/kinect_frames.launch">
    <arg name="camera" value="$(arg camera)" />
  </include>
}}}
となっており、この中にさきほどのkinectImage.launchにも出てきたrespawnやpublish_tfがあるのが、わかると思います。openni.launchで宣言していた
{{{
<arg name="respawn" default="$(arg bond)" />
}}}
{{{
  <arg name="publish_tf" default="true" />
}}}
にたいして、さきほどのargタグで新たに値を設定しようとしていたということです。
{{{
$(arg bond)
}}}
はいろいろ宣言されているargのvalueを取得するものです。この場合bondの値をとってます。
{{{
<arg name="respawn" default="$(arg bond)" />
}}}
では、その取得した値をrespawnのデフォルト値に設定しています。
また、
{{{
  <include if="$(arg publish_tf)"
	   file="$(find openni_launch)/launch/kinect_frames.launch">
    <arg name="camera" value="$(arg camera)" />
  </include>
}}}
でもあるように、取得した値で、includeタグの中を使うかどうかをif分での分岐の条件に指定したりすることにも使えます。