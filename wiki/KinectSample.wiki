#summary Kinectのサンプル
<wiki:toc max_depth="2" />

= 概要 =

ここで紹介するKinectのサンプルは，
現時点では、
 * jsk-ros-pkgのroseus_tutorialsにあるkinect.launch 
 * jsk-ros-pkgのroseus_tutorialsにあるkinect-model-viewer.l
を紹介します。

kinectの主なサンプルプログラム(launch)は、jsk-ros-pkg以下の
 * roseus_tutorialsのkinect.launch

jsk-ros-pkg以下のcppのサンプルは、
 * jsk-openni-kinect/openni_pointer/src/openni_pointer.cpp
 * jsk-openni-kinect/openni_scene/src/openni_scene.cpp
 * jsk-openni-kinect/openni_swipe/src/openni_swipe.cpp
/opt/ros/fuerte/stacks以下のcppのサンプルは、
 * openni-tracker/src/openni_tracker.cpp

jsk-ros-pkg以下のeusのサンプルは
 * roseus_tutorials/kinect-model-view.l
 * roseus_tutorials/openni-swipe.l
 * roseus_tutorials/openni-pointer.l

などがあると思いますので、適宜参照してみてください。

= 準備 =
 * [ROS_Install]にしたがってROSと講義関連パッケージがインストールされていることが前提です．
 * Kinectを使います。特に必要な設定はありません。Kinect For Windowsでは動作がどうなるか確認できておりません。
 * OSのバージョンによってはうまくいかないものもあります。

=kinect.launch=
== 実行 ==
２つのターミナルを立ち上げます
まずは，1つ目で
{{{
roscore
}}}
として下さい．

次に2つ目のターミナルで
{{{
roslaunch roseus_tutorials  kinect.launch
}}}
とします．

そうすると、小さな画面が表示され、デプスの値によって色が分けて表示されます。(Kinectの近くに手をざすとうまく動かないので、50cmほどkinectから離してかざしてみてください)





[http://rtm-ros-robotics.googlecode.com/svn/wiki/kinect_launch_imageView.png]


プログラムがうまくいく、手だけではなく全身を写す（上半身だけでも可）と、
実行した二つ目のターミナルに
{{{
[ INFO] [1352997345.849554969]: New User 1
[ INFO] [1352997349.105289471]: Pose Psi detected for user 1
[ INFO] [1352997349.571060727]: Calibration started for user 1
[ INFO] [1352997350.071415612]: Calibration failed for user 1
[ INFO] [1352997350.071990197]: Pose Psi detected for user 1
[ INFO] [1352997364.279894287]: Lost user 1
}}}
などの表示が見られると思います。（人によって表示される順番や事柄は変わります。）


次にrostopicはどのようなものがあるかを確認します。
以下のコマンドを新しいターミナルで実行してください。
{{{
rostopic list
}}}
を行ってください。
このサンプル以外のものを行っていなければ、大量に表示されるtopicの最後に/tfがあるのが確認できると思います。さらに、
{{{
rostopic echo /tf
}}}
を行うことでtfのtopicに今流れてきている値を確認できると思います。

一応確認するために、/tfのmsgの形は、
{{{
rostopic type /tf
}}}
で、/tfのメッセージのタイプがわかりその内容は
{{{
rosmsg show tf/tfMessage
}}}
で見れます。
{{{
geometry_msgs/TransformStamped[] transforms
  std_msgs/Header header
    uint32 seq
    time stamp
    string frame_id
  string child_frame_id
  geometry_msgs/Transform transform
    geometry_msgs/Vector3 translation
      float64 x
      float64 y
      float64 z
    geometry_msgs/Quaternion rotation
      float64 x
      float64 y
      float64 z
      float64 w
}}}
のようになります。

ここで、roslaunch roseus_tutorials  kinect.launch
を行ったターミナルを開きながら、カメラの前で肘を直角にして両腕をあげてジッとしてみてください。





[http://rtm-ros-robotics.googlecode.com/svn/wiki/kinect_sample_hangup_pose.png]

{{{
[ INFO] [1352998662.710511745]: Calibration started for user 3
[ INFO] [1352998664.822129074]: Calibration complete, start tracking user 3
}}}
などが表示されると思います。そうすると、rostopic echo /tfを行ったターミナルを開いているならば、/tfの値も変更されて、ターミナルに体の各部位の情報が表示される様になります。

ここで、tfでの値を3Dで表示しましょう。
また新しいターミナルで以下のコマンドを実行してください。
{{{
rosrun rviz rviz
}}}
起動後に、左側のコラムのなかにaddのボタンがあると思います。

以下のものを開いたウィンドウでaddしていってください。
 * rviz/Axes
 * rviz/Grid
 * rviz/Camera
 * rviz/TF
それぞれ、基準の座標、グリッド、カメラ、体の部位の表示を
表してくれます。

しっかりと、トラックが行われている状態であれば、
体の表示がrvizの中に見れると思います。



[http://rtm-ros-robotics.googlecode.com/svn/wiki/kinect_sample_rviz_view.png]

==ソース・概略==
roseus_tutorials/roskinect.launchの中を軽く見てます。
{{{
 <node name="openni_tracker" pkg="openni_tracker" type="openni_tracker" output="screen" respawn="true" >
    <param name="camera_frame_id" value="camera_depth_frame"/>
  </node>
  <node name="openni_dispairty_view" pkg="image_view" type="disparity_view">
    <remap from="image" to="/camera/depth/disparity" />
  </node>
  <node name="map_openni_camera" pkg="tf" type="static_transform_publisher"
        args="0 0 1.5 0 0 0 /map /camera_link 100" />
}}}
となっているのが分かると思います。

上から、nodeとして、pkg=""あたりで、openni_trackerと、image_viewと、tfに該当するものが、作られているのがわかります。openni_trackerは、roscd openni_trackerで分かると思いますが、/opt/ros/fuerte/stacksのなかにパッケージがあり、これは、ユーザーを検知に関するnodeです。image_viewは、depthの結果を表示するnodeです。tfは体の関節などの情報が送られるnodeです。

何となく、どのようなnodeを呼び出しているかは簡単には分かったと思います。
他にどのようなnodeが立ち上がるかを確認するには、新しいターミナルで
{{{
rosnode list
}}}
を実行します。先ほど上にかかれたopenni_tracker openni_disparity_view map_openni_cameraを含めいくらかのnodeが確認できると思います。

tfについての詳しいことは、ROSwikiの[http://www.ros.org/wiki/tf/Tutorials/Introduction%20to%20tf tf]を参照してください。



=kinect-model-viewer.l=
このサンプルは、kinect.launchで作られたtfの体各位の情報をとってきて、IRT Viewerで表示させるもので、上のrvizのような表示が見れます。
== 実行 ==
kinect.launchも実行する必要があるので、まず、それらを準備します。

{{{
roscore
}}}
{{{
roslaunch roseus_tutorials kinect.launch
}}}

これでtfのtopicに必要な情報が送られます。

そしたら次に、kinect-model-viewer.lを実行します。
{{{
rosrun roseus_tutorials kinect-model-viewer.l "(test-kinect)"
}}}
としてください。
これは、kinect-model-viewer.lをroseusを立ち上げて、(test-kinect)をその中で実行しています。
もし以下のエラーが出る人は、
{{{
roseus : command not found
}}}
[http://code.google.com/p/rtm-ros-robotics/wiki/ROS_Example_TroubleShooting#roseus_:_command_not_found_と言われる． 困ったときは]
を参考に.bashrcを更新してもう一度試して見てください。

実行したら、IRT Viewerに棒人間が表示されると思います。

[http://rtm-ros-robotics.googlecode.com/svn/wiki/kinect_sample_model_view.png]


==ソース・概略==
kinect-model-viewer.lの中身を簡単に見ていきます。

かなりコードは長いですが、コードの最後の方の一部のみを見ていきたいと思います。

{{{
#-:ros
(unless (find-package "ROS") (make-package "ROS"))
#+:ros
(defun test-kinect (&key robot fname loop-hook) ;; :fname "test-kinect.bvh", loop-hook is a function to call inside do-until-key loop
  (let ((floor (make-cube 2000 1000 10 :pos #f(1000 0 0))) f)
    (if fname (setq f (open fname :direction :output)))
    (ros::roseus "kinect_bvh")
    (if (not (boundp '*tl*))
        (setq *tl* (instance ros::transform-listener :init)))
    (setq b (make-kinect-bvh-robot-model))
    (objects (list floor b))
    (if robot (objects robot))
    ;;(defun find-node (name) (find name *arrows* :key #'(lambda (x) (send x :name)) :test #'string=))
    (if f (send b :dump-hierarchy f))
    (do-until-key
     (let ()
       (when (send b :copy-state-from-tf *tl*)
	 (format t "kinect robot : larm pos=~A, rarm pos=~A (world)~%"
		 (send b :larm :end-coords :worldpos)
		 (send b :rarm :end-coords :worldpos))
	 (format t "             : larm pos=~A, rarm pos=~A (local)~%"
		 (send (send (send b :torso :end-coords) :transformation (send b :larm :end-coords)) :worldpos)
		 (send (send (send b :torso :end-coords) :transformation (send b :rarm :end-coords)) :worldpos)))
       (when robot
	 (send b :copy-state-to robot))
       (if (functionp loop-hook) (funcall loop-hook))
       (send (get *viewer* :pickviewer) :look-all)
       (x::window-main-one)
       (if f (send b :dump-motion f))
       )) ;; do-until-key
    ))

;; (test-kinect)
}}}

ソースの上半分は、この上のコードで使用するためのクラスの定義や、tfからの情報の取得や反映をしてくれるものを実装してあり、このtest-kinectはそれらの関数を用いたプログラムを書いてます。

一行ずつ見ていきます。

{{{
#-:ros
(unless (find-package "ROS") (make-package "ROS"))
#+:ros
}}}
この#-:rosは「rosがないならば次の（で始まるものを実行する」という意味で、#+:rosは素の反対に「rosがあるならば次の（で始まるものを実行する」という意味になります。

{{{
(if fname (setq f (open fname :direction :output)))
    (ros::roseus "kinect_bvh")
    (if (not (boundp '*tl*))
        (setq *tl* (instance ros::transform-listener :init)))
    (setq b (make-kinect-bvh-robot-model))
    (objects (list floor b))
}}}

ここでは、bの棒人間の準備をしたり、必要なものを準備してます。読み飛ばして構いませんが、ここで、IRT Viewerの表示するものも用意していることだけは知っておいてください。




==実機でやるときは==
実機のpr2でコードを動かす際は以下の事に注意してください。
 * kinect-model-viewer.lの中の、座標系が、デフォルトでmapになっており、kinectでテストしているときは、kinect真下を座標の原点にしてますが、pr2でのmapはpr2の足元が原点とはなりません。/mapとなっているところはすべて/base_footprintに変えてください。
 * kinect.launchをpr2の方で起動する必要がありますので、kinect.launchがpr2で起動していることを確認してください。