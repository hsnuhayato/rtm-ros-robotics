#summary One-sentence summary of this page.

= 10/26 =
 * ARマーカをノートPCで使う
  * カメラが /dev/video0で利用できるとする
   * xawtv -C /dev/video0 などでキャプチャできていればOK
  * マーカ定義ファイルを ar_pose パッケージの data ディレクトリにコピー
{{{
  $ roscd Sense  # agentsystem_hironx_samples/Senseがなければ svn up
  $ cp data/object_4x4_0.024 `rospack find ar_pose`/data/ # ar_poseをdebパッケージでインストールしている場合はsudoが必要
  $ roslaunch Sense ar_pose.launch
  $ rostopic echo /ar_pose_marker # 別ターミナルで実行
}}}

 * デフォルトのカメラパラメータはHIROのハンドカメラのものなので、正しい位置を出すには利用しているカメラのパラメータを記入する必要がある

 * カメラキャリブレーション
{{{
 $ rosrun camera_calibration cameracalibrator.py --size 7x10 --square 0.025 image:=/usb_cam/image_raw
}}}
  * チェッカーボードの格子の数、マスの大きさはキャリブレーションに利用するボードにあわせる

  * 出力された /tmp/calibrationdata.tar.gz を解凍してできる ost.txt の数値を agensystem_hironx_samples/Sense/ar_pose.launch に書き入れる

 * 複数マーカ認識のar_multiはmarker_width, marker_center_x, marker_center_yなどのparameterを見ない。
 * data/object_4x4などのマーカ定義ファイル中の記述を参照する。
 * マーカデータファイルは ar_pose/data/4x4 の中にgif,psがあるので、これを印刷して使う。
 * マーカ周囲の白枠は重要。これがないと正しく認識されない。


= 11/01 =

 * 目標：grasp one piece, put in container

 * 自分のマシンをHIRONXネットワークに入れる
 
  * ネットワークを設定する
   * IPアドレスとgateway, ネームサーバを設定
  * roscoreの場所を指定する
   * export ROS_MASTER_URI=http://VisionPC:11311 
  * rostopic list で確認

 * サンプルプログラム
  * iv_plan/examples/pick_piece.py
  * rr.connect()
   * rr.connect()をファイル中に書いてimportすると固まる問題があるので注意
  * detect_pose() # pieceを認識出来る姿勢に移動
  * demo() # 黄緑piece（marker 2）を見るけ、水色piece（marker7）の上にのせる

 * HIRONX実機とのインタフェース
  * RT-componentでシステムは作られている
  * 基本的にpython上でrtc-handleを使うか、コマンドラインでrtshellを使う
   * iv_plan/src/hiro_controller.py, iv_plan/src/setup_rtchandle.py
  * ロボットの関節角の取得
   * rr.get_joint_angles()
  * 力センサの値を読む
   * rr.get_force_data()
  * ロボットへの関節角指令の送信
   * rr.send_goal(目標姿勢と時間) # 引数の例はpick_piece.pyを参照

  * 下位レベルのインタフェース
   * RobotHardware0.jointDatOutデータポートを読む
   * force....データポートを読む
   * seqへの通信を仲介するjython上のサーバにソケットでコマンドを送る


 * Pythonプログラムでのセンサデータの受信
  * ROSのtopicに対してcallbackを登録する
  * 例(iv_plan/examples/pick_piece.py)：
{{{
# -*- coding: utf-8 -*-

import roslib; roslib.load_manifest('iv_plan')
import set_env
from ivutils import *
from hiro_controller import *

import rospy
from ar_pose.msg import ARMarkers
from tf.transformations import *

class MyRobotInterface(HIROController):
    def __init__(self, nameserver):
        HIROController.__init__(self, nameserver)
        self.lhand_markers = []

    def connect(self):
        HIROController.connect(self)
        rospy.init_node('pick_piece')
        rospy.Subscriber('/hiro/lhand/ar_pose_marker', ARMarkers, self.lhand_callback)

    def lhand_callback(self, msg):
        if len(msg.markers) > 0:
            self.lhand_markers = msg.markers

    def recognize(self, camera='lhand', thre=1.5):
        def parse_marker(marker):
            if rospy.Time.now().to_sec() - marker.header.stamp.to_sec() > thre:
                return None
            else:
                p = marker.pose.pose.position
                trans = 1000.0 * array([p.x, p.y, p.z])
                q = marker.pose.pose.orientation
                rot = [q.x, q.y, q.z, q.w]

                return (marker.id,
                        FRAME(mat=MATRIX(mat=quaternion_matrix(rot)[0:3,0:3].tolist()),
                              vec=VECTOR(vec=(trans.tolist()))))

        if camera == 'lhand':
            return filter(None, [parse_marker(m) for m in self.lhand_markers])
        else:
            return filter(None, [parse_marker(m) for m in self.rhand_markers])


rr = MyRobotInterface(set_env.nameserver)
rr.connect()

}}}


= Recognition =

* HIRONXハンドカメラのキャプチャノードとマーカ認識器の起動

vision@VisionPCで、
{{{
 $ roslaunch Sense sense_hiro_ar.launch
 $ rostopic list

/hiro/lhand/ar_pose_marker
/hiro/lhand/usb_cam/camera_info
/hiro/lhand/usb_cam/image_raw
/hiro/lhand/usb_cam/image_raw/compressed
/hiro/lhand/usb_cam/image_raw/compressed/parameter_descriptions
/hiro/lhand/usb_cam/image_raw/compressed/parameter_updates
/hiro/lhand/usb_cam/image_raw/theora
/hiro/lhand/usb_cam/image_raw/theora/parameter_descriptions
/hiro/lhand/usb_cam/image_raw/theora/parameter_updates
/hiro/lhand/visualization_marker
/hiro/rhand/ar_pose_marker
/hiro/rhand/usb_cam/camera_info
/hiro/rhand/usb_cam/image_raw
/hiro/rhand/usb_cam/image_raw/compressed
/hiro/rhand/usb_cam/image_raw/compressed/parameter_descriptions
/hiro/rhand/usb_cam/image_raw/compressed/parameter_updates
/hiro/rhand/usb_cam/image_raw/theora
/hiro/rhand/usb_cam/image_raw/theora/parameter_descriptions
/hiro/rhand/usb_cam/image_raw/theora/parameter_updates
/hiro/rhand/visualization_marker
/rosout
/rosout_agg
/tf

 $ rostopic echo /hiro/lhand/ar_pose_marker
 $ rostopic echo /hiro/rhand/ar_pose_marker
 $ rostopic echo /tf

}}}


= ROS how to =
 * ROS_MASTER_URIの設定例
  * export ROS_MASTER_URI=http://lupus:11311
 * rostopic, rxgraph, rosrun, roslaunch ...


= RT-middleware how to =
 * rtshell, rtc-handle, system editor


= HIRONX実機の使い方 =
 * 起動
  * 本体の電源が先、VisionPCが後
  * 本体の電源を入れるとすぐにビジョンPCの電源を入れて良い
  * しばらく待って、表示灯が緑、白の点滅になれば起動完了
   * (注意点) 以下のメッセージで起動が止まることがある。この場合は、電源ボタン長押しで一度電源を切り、再度電源投入すると起動する。
    * Could not determine which IOAPIC pin timer is connected too 

 * 準備(VisionPCで)
{{{
 $ cd /opt/grx/HIRONX/script
 $ ./gui.py
 # ウィンドウ上部の緑丸のアイコンでも起動できる。
}}}

  # setup rt-system
   * RTCの接続とRobotHardware以外のRTCでロードされていないものをロード
  # calibrateJoint
   * キャリブレーション、ロボット起動後に一回行う
    * ボタンを押しても、キャリブレーションが始まらない、変な姿勢で終わることがある。この場合、一度servoOffして、再度 calibratejointを行う。関節リミットから遠ざけると上手くいくことが多い？エラーが起きたときなど、calibrateJointをしても始まらない場合がある。この場合はロボットの再起動。 

 * 動作テスト (jythonスクリプト)
  # goInitial (初期姿勢へ移行)
  # testPattern (テスト動作実行)
   * 関節位置指令
  # ikTest (IKのサンプル), ikTest_spline (手先軌道を空間的に滑らかにつなぐサンプル) 
   * これらは手先位置指令

 * 終了手順
  # gui.pyのウィンドウで、goOffPoseを押して終了姿勢に移動するまで待つ。
  # 終了姿勢になるとservoOffするかどうか訊かれるので、了解ボタンを押す。
  # shutdownボタンを押す。
   * RTC経由で本体のシャットダウン
   * sshで直接ログインしてshutdownする場合は以下のようにする。
{{{
  $ ssh hiro@hiro014
  $ su
  # /opt/grx/bin/shutdown.sh 
  (注意) shutdownコマンドはリブートしたいときに使う
}}}
  # VisionPCの電源を切る
{{{
  $ sudo shutdown -h now
}}}

 * 再起動の手順
  # ロボット本体の再起動
{{{
  $ ssh hiro@hiro014
  $ su
  # shutdown
  ;; ハブの電源供給は止まらないので、再起動のときはVisionPCの再起動は不要
}}}

 * 保護停止、非常停止
  * 保護停止は指令値をクリア、解除するとそのまま次の動作を実行できる。
  * 非常停止はサーボオフする。
   * 解除後、GUIで一度servoOffボタンをして再度servoOnを押すことで、たいてい復帰できる。復帰できなければ、ロボットを再起動する。


* 画像の保存（ROS）
 * rosrun image_view image_view image:=/hiro/rhand/usb_cam/image_raw
  * ウィンドウ上で右クリック 
 * rosrun image_view extract_images image:=/hiro/rhand/usb_cam/image_raw


 * 認識結果をロボットのモデルを用いて世界座標に変換し，pieceを把持する例(iv_plan/examples/pick_piece.py)


= 11/16 =

 * 目標：environment with multiple pieces, grasp one piece, put in container

 * HIRONXネットワークの変更
  * VisionPCでHIROネットワーク用のdhcp,dnsサーバを立てた
  * dhcp設定でネットワークに接続すると，dhcp経由でDNS,gateway,domain等の情報を自動で取得する
  * 正しく通信できているかどうかのテスト

{{{
 $ pint hiro014
 $ export ROS_MASTER_URI=http://VisionPC:11311
 $ rostopic list
 $ rostopic echo /hiro/rhand/ar_pose_marker
}}}


 * カメラ座標系から世界座標系への座標変換RT-component (CoordTransComp.py)
  * 入力・出力ともに4x4行列を一次元配列にしたもの
  * 認識に使ったカメラ、認識結果、ロボット位置、ロボットの関節角度を引数として、認識結果を世界座標系に変換する

{{{
 適当なディレクトリで作業するための準備
 $ cd <適当なディレクトリ>
 $ cp `rospack find iv_plan`/src/rtc.conf .
 mytest.pyに適当なコードを書く

 $ ipython mytest.py

 rr.connect()

 # AR marker位置・姿勢結果の取得
 pose = rr.recognize('rhand')[0][1] 

 # 世界座標に変換（適宜，データ表現の変換処理が入っている）
 ctsvc.ref.Query('rhandcam', pose2mat(pose), robotframe, rr.get_joint_angles())
}}}

 * mytest.py
{{{
# -*- coding: utf-8 -*-

import roslib; roslib.load_manifest('iv_plan')
from hironx_if import *
}}}

 * 開発中サンプルの場所
  * agentsystem_hironx_samples/semi/


 * 各ピースの座標系(cubeassembly.py)
  * cubeassembly.py を実行して，self.ShowGoal(eye(4)) をして確認する
  * 3x3x3に組み上がった状態で共通の原点を持つようになっている

{{{
 $ python cubeassembly.py
}}}

 * OpenRAVEとHIRONX実機の接続
  * 実機の関節角度を読み，OpenRAVEのモデルに反映させる
  * 関節の順番を調整する

{{{
# -*- coding: utf-8 -*-

import roslib; roslib.load_manifest('iv_plan')
from hironx_if import *

rr.connect()

# openravepy
from openravepy import *
env=Environment()
env.SetViewer('qtcoin')
env.Load('robots/kawada-hironx.zae')
orrobot=env.GetRobots()[0]

# write your own code

orderednames = ['CHEST_JOINT0', 'HEAD_JOINT0', 'HEAD_JOINT1', 'RARM_JOINT0', 'RARM_JOINT1', 'RARM_JOINT2', 'RARM_JOINT3', 'RARM_JOINT4', 'RARM_JOINT5', 'LARM_JOINT0', 'LARM_JOINT1', 'LARM_JOINT2', 'LARM_JOINT3', 'LARM_JOINT4', 'LARM_JOINT5', 'RHAND_JOINT0', 'RHAND_JOINT1', 'RHAND_JOINT2', 'RHAND_JOINT3', 'LHAND_JOINT0', 'LHAND_JOINT1', 'LHAND_JOINT2', 'LHAND_JOINT3']
ordertortc = array([orrobot.GetJoint(name).GetDOFIndex() for name in orderednames],int32)
ordertoopenrave = [orderednames.index(j.GetName()) for j in orrobot.GetJoints()]
orrobot.SetDOFValues(array(rr.get_joint_angles())[ordertoopenrave])

}}}

 * HIRONX実機のコントローラ
  * 腰首、右腕、左腕、右手、左手の5個の配列
  * 動かさないところは空でよい
  * 第3引数は動作に要する時間（目標値）
  * 第4引数は動作終了まで待つかどうか（Trueなら待つ）

{{{
 $ ipython mytest.py
 $ rr.connect()
 q = rr.get_joint_angles()
 q[0] += 0.2
 rr.send_goal([q[0:3],[],[],[],[]], 3.0, True)
}}}


  
 * openraveの軌道を変換する [http://openrave.org/en/main/openravepy/openravepy_int.html#openravepy.openravepy_int.Trajectory Trajectory関数参照], [http://openrave.org/en/main/openravepy/openravepy_int.html#openravepy.openravepy_int.ConfigurationSpecification ConfigurationSpecification関数参照]

{{{
# -*- coding: utf-8 -*-

import roslib; roslib.load_manifest('iv_plan')
from hironx_if import *

# openravepy
from openravepy import *
env=Environment()
env.SetViewer('qtcoin')
env.Load('robots/kawada-hironx.zae')
robot=env.GetRobots()[0]
rr.connect()

# write your own code

orderednames = ['CHEST_JOINT0', 'HEAD_JOINT0', 'HEAD_JOINT1', 'RARM_JOINT0', 'RARM_JOINT1', 'RARM_JOINT2', 'RARM_JOINT3', 'RARM_JOINT4', 'RARM_JOINT5', 'LARM_JOINT0', 'LARM_JOINT1', 'LARM_JOINT2', 'LARM_JOINT3', 'LARM_JOINT4', 'LARM_JOINT5', 'RHAND_JOINT0', 'RHAND_JOINT1', 'RHAND_JOINT2', 'RHAND_JOINT3', 'LHAND_JOINT0', 'LHAND_JOINT1', 'LHAND_JOINT2', 'LHAND_JOINT3']
ordertortc = array([robot.GetJoint(name).GetDOFIndex() for name in orderednames],int32)
ordertoopenrave = [orderednames.index(j.GetName()) for j in robot.GetJoints()]
robot.SetDOFValues(array(rr.get_joint_angles())[ordertoopenrave])

manip=robot.SetActiveManipulator("rightarm_torso")
ikmodel=databases.inversekinematics.InverseKinematicsModel(robot,freeindices=manip.GetArmIndices()[:-6])
if not ikmodel.load():
    ikmodel.autogenerate()
    
def recognize():
    pose = rr.recognize('rhand')[0][1]
    f = ctsvc.ref.Query('rhandcam', pose2mat(pose), robotframe, rr.get_joint_angles())
    f = reshape(f, (4,4))
    f[0:3,3] /= 1000 # hand goal
    return f

def pick(Tgoal):
    # openravepy
    basemanip = interfaces.BaseManipulation(robot)
    trajdata = basemanip.MoveToHandPosition(matrices=[Tgoal],execute=False,outputtraj=True)
    traj = RaveCreateTrajectory(env,'').deserialize(trajdata)
    return traj

def execute(traj):
    robot.GetController().SetPath(traj)

    rtcvalues = spec.ExtractJointValues(data,robot,ordertortc,0)
    spec.ExtractDeltaTime(data)
    
    waypointoffset=spec.GetGroupFromName("iswaypoint").offset
    if data[waypointoffset]:
        print "is waypoint"
    else:
        print "can be ignored"
    print traj.GetNumWaypoints()
    print traj.GetWaypoint(0)

}}}



= 11/23 =

 * 目標：environment with multiple pieces, grasp one piece, put in container
   （先週のつづき）

 * AR markerでピースの姿勢認識
  * marker->pieceの座標変換を入れて，piece姿勢の認識を可能とする
  * 認識結果をシミュレータ内に反映させる
  * cubeassembly.pyとの整合性をとる
  * 適宜、実機で正しく認識できるかどうか確認する

 * OpenRAVEとHIRONX実機の接続
  * cubeassembly.pyのMoveHandPositionなどの位置で，実機にコマンドを送るコードを入れる
  * 簡単な軌道を生成し、出力された軌道を実機に送ってみる
  * 手が台車部に当たらないようにモデルを修正し、実機で実行可能な軌道を生成する
  * できるだけ容易に把持可能なシーンを作成する
  * 環境中に箱を入れる
  * 机上の複数のpieceを探す動作を実装する

 * 実機で掴みやすい配置のpieceを１つずつ把持し，箱に入れる


= 今後の目標 =

 * Nicely-ordered pieces (easy-solution exists). Put in goal (cube) configuration
 * Randomly-ordered pieces (hard). Put in goal (cube) configuration
 * Put complex obstacles and detect them with the Kinect.
 * Make it very fast


= その他 =

* 画像によるピース認識

 * 平面的なピース認識(yellow,red,brown,aqua,yellowgreen)
{{{
 roscd piece_recog
 rosmake
 rosrun piece_recog piece_recog_node image:=/hiro/lhand/usb_cam/image_raw
 rosservice call /set_target_piece yellow  # サービスで認識対象を設定
 rostopic echo /tf  # tfが出力される
 rosrun rviz rviz  # /camera->/piece を表示してみる
}}}


* Kinectによる障害物（対象物以外の環境）認識

 * camera_infoは設定ファイルから読み込めるように作られていないので、openni_nodelet.cppを直接編集して書きこむ (diamondback)

* ステレオ視
 