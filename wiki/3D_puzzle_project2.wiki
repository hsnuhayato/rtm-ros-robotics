#summary One-sentence summary of this page.


= 11/30 =

* 目標：environment with multiple pieces, grasp one piece, put in container
   （先週のつづき）

 * AR markerでピースの姿勢認識
  * marker->pieceの座標変換を入れる
  * 適宜、実機で正しく認識できるかどうか確認する
  * 机上の複数のpieceを探す動作を実装する
   * 机上をスキャンし、7つのpieceを正しい姿勢でシミュレータ内に配置するプログラムを作成する

 * OpenRAVEとHIRONX実機の接続
  * 容易に把持可能なシーンを作る
  * 実機で掴みやすい配置のpieceを１つずつ把持し，箱に入れるプログラムを作成する
  * 把持プランナを使う
  * cubeassembly.pyのgraspAndPlaceObjectで生成した動作を実機で実行できるようにする
  * 手が台車部に当たらないようにモデルを修正し、実機で実行可能な軌道を生成する


* サンプルプログラム(mytest3.py)

 * pieceの認識結果のOpenRAVE環境に反映する(recognize)
  * ARマーカ=>pieceのtransformは考慮していない(認識したmarker位置へpieceを置くだけ)
 * 作り込みの把持位置(手首位置・姿勢)へ左手を動かす軌道を生成(plan)
 * 生成した軌道を実行(execute)
  * 引数realrobot=Trueで実機に動作指令を送る

{{{
# -*- coding: utf-8 -*-

import roslib; roslib.load_manifest('iv_plan')
from hironx_if import *

from cubeassembly import *

env=Environment()
env.SetViewer('qtcoin')
env.Load('data/hironxtable.env.xml')
robot=env.GetRobots()[0]
rr.connect()

orderednames = ['CHEST_JOINT0', 'HEAD_JOINT0', 'HEAD_JOINT1', 'RARM_JOINT0', 'RARM_JOINT1', 'RARM_JOINT2', 'RARM_JOINT3', 'RARM_JOINT4', 'RARM_JOINT5', 'LARM_JOINT0', 'LARM_JOINT1', 'LARM_JOINT2', 'LARM_JOINT3', 'LARM_JOINT4', 'LARM_JOINT5', 'RHAND_JOINT0', 'RHAND_JOINT1', 'RHAND_JOINT2', 'RHAND_JOINT3', 'LHAND_JOINT0', 'LHAND_JOINT1', 'LHAND_JOINT2', 'LHAND_JOINT3']
ordertortc = array([robot.GetJoint(name).GetDOFIndex() for name in orderednames],int32)
ordertoopenrave = [orderednames.index(j.GetName()) for j in robot.GetJoints()]
robot.SetDOFValues(array(rr.get_joint_angles())[ordertoopenrave])

manip=robot.SetActiveManipulator("leftarm_torso")
ikmodel=databases.inversekinematics.InverseKinematicsModel(robot,freeindices=manip.GetArmIndices()[:-6])
if not ikmodel.load():
    ikmodel.autogenerate()

self = CubeAssembly(robot)
self.CreateBlocks()


def recognize(camera='lhand'):
    pose = rr.recognize(camera)[0][1]
    f = ctsvc.ref.Query(camera+'cam', pose2mat(pose), robotframe, rr.get_joint_angles())
    f = reshape(f, (4,4))

    relvec = array([150,0,0]) + array([-450,0,-800])
    f[0:3,3] += relvec
    f[0:3,3] /= 1000

    self.gmodels[4].target.SetTransform(f)
    return f

def plan(f):
    g = eye(4)
    g[0:3,3] = [0.03,-0.03,0.12]
    f = dot(f,g)
    h = dot(dot(f[0:3,0:3], rotationMatrixFromAxisAngle([1,0,0],pi)), rotationMatrixFromAxisAngle([0,0,1],-pi/2))
    f[0:3,0:3] = h
    Tgoal = f

    basemanip = interfaces.BaseManipulation(robot)
    trajdata = basemanip.MoveToHandPosition(matrices=[Tgoal],execute=False,outputtraj=True)
    traj = RaveCreateTrajectory(env,'').deserialize(trajdata)

    return f, traj

def execute(traj, realrobot=False):
    robot.GetController().SetPath(traj)
    robot.WaitForController(0)
    robot.GetController().Reset(0)

    spec = traj.GetConfigurationSpecification()
    n = traj.GetNumWaypoints()
    for i in range(n):
        data = traj.GetWaypoint(i)
        rtcvalues = spec.ExtractJointValues(data,robot,ordertoopenrave,0)
        dt = spec.ExtractDeltaTime(data)
        print dt
        print rtcvalues
}}}


= 今後の目標 =

 * Nicely-ordered pieces (easy-solution exists). Put in goal (cube) configuration
 * Randomly-ordered pieces (hard). Put in goal (cube) configuration
 * Put complex obstacles and detect them with the Kinect.
 * Make it very fast


= その他 =

* 画像によるピース認識

 * 平面的なピース認識(yellow,red,brown,aqua,yellowgreen)
{{{
 roscd piece_recog
 rosmake
 rosrun piece_recog piece_recog_node image:=/hiro/lhand/usb_cam/image_raw
 rosservice call /set_target_piece yellow  # サービスで認識対象を設定
 rostopic echo /tf  # tfが出力される
 rosrun rviz rviz  # /camera->/piece を表示してみる
}}}


* Kinectによる障害物（対象物以外の環境）認識

 * camera_infoは設定ファイルから読み込めるように作られていないので、openni_nodelet.cppを直接編集して書きこむ (diamondback)

* ステレオ視

* HiroNX制御RTC(Act共通インタフェース)
 * OpenRTM-javaでRTC実装された制御モジュール
 * 今後、こちらに置き換えていく
 * フラットな関節角度ベクトルを引数に与える
{{{
 q = rr.get_joint_angles()
 q[0] += 0.2
 rr.send_goal(q, 3.0, True)
}}}
 