##
## チュートリアル
##


# 0, はじめに

$ roscd MotionPlan/src
$ ipython demo.py


# 1, pythonを使う

dir(r) # 関数やメソッドの一覧
r. [TAB] # メソッド名等の補間（ipythonの機能）
help(r) # 引数や説明

# とりあえずデモ

putbox() # 箱を認識して掴み、置き直す
putbox(name='box1')
demo() 

demo2() # 左手でのアプローチ

qs = gen_traj() # 手先でsinカーブを描くための関節角軌道を作る
play_traj(qs) # 作った軌道を実行する

# コマンドライン
# Ctrl+c
# Ctrl+p / Ctrl+n
# Ctrl+r


# 2, 環境中の物体操作

env
env.get_objects() # 環境中の物体一覧
[x.name for x in env.get_objects()] # 物体名一覧

# 物体は名前で管理されている。
# 同じ名前の物体は追加できないので、一度削除するか、名前を変えて追加する。
putbox() # テーブル上に箱を置く
b = env.get_object('box') # 名前で物体取得
put_box(name='box2') # 名前を変えると違う物体
env.delete_object('box2')

frm = b.where() # 物体の位置と姿勢を取得

# ロボットの移動、回転 ( world=>basejointの座標変換の変更 )
r.go_pos(-150,500,0) # 2D座標, (x,y,theta)
r.go_pos(-150,0,pi/2) # 横を向く


# 3, 自作サンプルの書き方
emacs mysample.py

== mysample.py ==
from demo import *

... 適当なコード ...
== ==

ipython mysample.py

# サンプルを修正後は、以下を実行すると
# mysample.pyの修正が反映される

import mysample # 最初の一回だけ

reload(mysample)
from mysample import *


# 4, ロボットの姿勢の操作

# joint, linkの取得
r
r.get_joints()
r.get_links()

# ジョイント名の取得
j0 = r.get_joints()[0]
dir(j0)
j0.angle
j0.name
map(lambda x: x.name, r.get_joints())
[x.name for x in r.get_joints()]

# 関節角度の表示
r.get_joint_angles()
# これは下のコードと同じ
[x.angle for x in r.get_joints()]

# 腕だけ
r.get_arm_joint_angles()
r.get_arm_joint_angles(arm='left')
r.set_arm_joint_angles(angles)
r.set_arm_joint_angles(angles, arm='left')

# ハンドだけ
r.get_hand_joint_angles()
r.get_hand_joint_angles(hand='left')
r.set_hand_joint_angles(angles)
r.set_hand_joint_angles(angles, hand='left')

# 関節１つを変える
r.get_joint_angle(0) # ID
r.set_joint_angle(0, 0.5) # IDと角度[rad]

# 初期姿勢に戻す
r.reset_pose()

# あらかじめいくつかの姿勢が定義されている
r.poses
r.poses.keys()

# r.reset_pose()は以下と同じ
r.set_joint_angles(r.poses['init'])

# 手のポーズ
r.hand_poses
r.hand_poses['open']
r.set_hand_joint_angles(r.hand_poses['open']) # 手を開く
r.set_hand_joint_angles(r.hand_poses['close']) # 手を閉じる

# アニメーション（首を振ってみる）
arange(0, 1, 0.2)
for th in arange(0,1,0.2):
	r.set_joint_angle(1, th)
	time.sleep(0.5)

# ちなみに左右の腕の関節書く取得は以下のコードと同じ
r.get_joint_angles()[3:9]
r.get_joint_angles()[15:19]

## 練習：
##   いろんな姿勢を作ってみよう
##   いろんな動作列を作って、アニメーションをしてみよう


# 5, 座標系(frame)について
# 3x3回転行列と3次元ベクトル = 4x4の同次数行列
# euler角, 自由軸回転, quaternion

help(VECTOR)
help(MATRIX)
help(FRAME)

# 値の生成(constructor)
VECTOR()
MATRIX()
FRAME()
v=VECTOR(vec=[100,0,0])
MATRIX(angle=pi/2, axis=VECTOR(vec=[0,0,1]))
MATRIX(c=pi/2) # a,b,cを同時に指定できないので注意
m=MATRIX([[1,0,0],[0,1,0],[0,0,1]])
FRAME()

frm.mat # 姿勢部分
frm.vec # 位置部分

# 演算
v*v # ベクトル積
dot(v,v) # 内積
v+v # 和
2*v # スカラー倍
# 行列は姿勢表現専用（直行行列）
m*m # 積
-m # 逆行列
# -mはじ実装は転置行列
# スカラー倍、行列和は定義されない(配列の結合解釈される)

# 姿勢表現間の変換
m.abc() # 行列=>euler
m.rot_axis() # 行列=>自由軸回転
# euler=>回転行列、自由軸回転=>回転行列は上記のMATRIXのコンストラクタ

# 位置と姿勢を合わせた表現（同時行列）
FRAME(mat=m, vec=v)
FRAME(xyzabc=[x,y,z,a,b,c])

f=FRAME(vec=[500,0,1000])
show_frame(f)
f.mat = MATRIX(a=pi/4)
show_frame(f)
f.mat = MATRIX(a=pi/4)*MATRIX(b=pi/4)
show_frame(f)

# 座標系の親子構造
FRAME.affix() # 座標系の親子関係の定義
FRAME.unfix() # 座標系の親子関係の削除
FRAME.set_trans() # 親子間の座標変換の設定
f.rel_trans # 親子間の座標変換の取得

# 物体追加
# putbox()の記述を参照
# 表示用形状とFRAMEを作り、適当な親座標系の子FRAMEとして、座標系ツリーに挿入する

env.insert_object('box2') # 物体追加
env.delete_object('box2') # 物体削除(子フレームの物体も削除される)

## 練習：
##  物体の位置を変えてみよう
##  テーブルを20[cm]前に出してみよう
##  ロボットを移動させてみよう
##  適当な物体を作成したり、削除してみよう


# 6, 逆運動学(Inverse Kinematics, IK)の利用

# 手首位置，姿勢の取得
r.fk() # 順運動学計算，現在の手先FRAMEを返す．デフォルトは右手．
r.fk(arm='left') # 左手は明示的に引数で指定する

# 目的位置へのアプローチ

putbox() # 手が届きそうなところに置く
objfrm = detect()

# アプローチ姿勢，把持姿勢の計算（2つずつ求まる）
afrms, gfrms = pl.grasp_plan(objfrm)

# アプローチ姿勢を表示
# (FRAMEからCoordinateObjectを作って、環境にinsert_objectする)
show_frame(afrms[0])
show_frame(afrms[1])

# 目標位置へ手を伸ばすための関節角度を計算する
r.ik(afrms)
help(r.ik)
# - IKは目標手先FRAMEの集合を引数にとり、
#   初期姿勢から関節空間での重み付き距離順に並べた解の列を返す
#   目標手先フレーム１つを引数に指定してもよい
# - 解が存在しなければNone
# - ほとんどの場合、最初の解を採用すればよい

asols = r.ik(afrms)
r.set_arm_joint_angles(asols[0]) # 腕の角度のみ

gsols = r.ik(gfrms)
r.set_arm_joint_angles(gsols[0]) # 腕の角度のみ

# 腰を使う
# 腰yaw軸を使うと手の届く範囲が大きく広がる

asols = r.ik(afrms, use_waist=True) # 返値の形式が違うので注意 (waist_yaw, arm_angles)
th,js = asols[0]
r.set_joint_angle(0,th) # 腰を回す
r.set_arm_joint_angles(js) # 腕の姿勢を変える

# 他の解も表示してみる
for th,js in asols:
        r.set_joint_angle(0,th)
        r.set_arm_joint_angles(js)
        time.sleep(0.5)

# 物体をハンドに固定する
tgtobj = env.get_object('box0')
handjnt = r.get_joint('RARM_JOINT5')
reltf = (-handjnt.where())*tgtobj.where()
tgtobj.unfix()
tgtobj.affix(handjnt, reltf)

# 手先軌道での動作生成 (収束IKではなく、初期姿勢に近い解を明示的に選択している)

# 軌道の作成
traj = CoordinateObjects('trajectory')
traj.append( ... ) # 適当なフレームを追加する
env.insert_object(traj, FRAME(), env.get_world()) # 世界座標相対で軌道を定義
traj.coords

# 削除したいときは
env.delete_object('trajectory')


## 練習：
##   物体位置を変えて手先が目標位置へいくことを確認しよう
##   適当な軌道を作って追従させてみよう


# 7, 実機を動かす

# 実機とのインタフェース

rr = RealHIRO()
rr.connect() # 認識処理が始まっていなければ開始指令を送る
rr.get_joint_angles() # tf and joint state by ROS
js = r.get_joint_angles()
# socket(+pickle) to jython script
# scequencer, wait
duration = 4.0
rr.send_goal(js, 4.0) # blocking
rr.send_goal(js, 4.0, wait=False) # non-blocking
# blockingな呼出しも最大10[sec]でtimeoutする仕様

# 実際には、モデルで姿勢を確認してから以下を実行するのが便利
sync() # デフォルトは4[sec]で実機をモデルに同期させる
sync(duration=3.0) # 時間を変える

# チェッカーボードの認識
detect() # 認識結果がboxとして表示される
objfrm = detect() # 実機の場合はROSの識別器からtfを受信
                  # simulationの場合は直接、物体位置を読む

# Linkに沿った座標変換
# 認識結果は、カメラ=>対象物の座標変換であるので、
# 世界座標=>対象物の座標変換に直す
# detect()関数の中で行っている処理
r.Thd_leye # 頭リンク => 左目カメラへの変換
Tleye_obj = rr.detect()
Twld_hd = r.get_link('HEAD_JOINT1_Link').where()
Twld_obj = Twld_hd * r.Thd_leye * Tleye_obj

# ハンドカメラの利用
# 右手ハンドカメラで2個の箱を探し、
# マーカ1番(隅が欠けている方)の箱を2番の箱の上に置く

# 実機の場合
hand_cam_demo()

# シミュレータで実行するときは，先に手が届く場所に箱を置いておく
# 'box0' => marker 0, 'box1' => marker 1 に対応
putbox(name='box0', vaxis='y')
putbox(name='box1', vaxis='y')
hand_cam_demo()

# もう一度実行したいとき
env.delete_object('box0')
putbox(name='box0', vaxis='y')


# ハンドカメラはAR-toolkitによるmarker認識(複数物体対応)
# 返り値は (マーカ番号、カメラ=>マーカの座標変換)のリスト
# non-blockingで最新の認識結果を返す
# 過去 thre [sec]以内に認識に成功していないマーカは返さない
# デフォルトは thre = 0.5[sec]

# Linuxのドライバの問題で1台のPCで2つのハンドカメラデバイスを開けない
# 2台のPCに1つずつつないでいる
# 左手カメラを使うときには、まず左手カメラのキャプチャと認識プログラムを走らせる必要がある
lupus@ roslaunch Sense sense_lhand_ar.launch
rr.detect(camera='lhand') # 左手カメラによる認識

# 平行グリッパの間隔指定把持
r.grasp(width=65, hand='right') # 指の間隔がwidth[mm]になるように指を動かす
r.grasp(65) # これでも同じ

# アプローチ距離の指定
pl.reaching_plan(objfrm, approach_distance=80) # 少し遠くからアプローチ


# 練習：
#  手や首を動かして、目の前以外に置かれた箱を掴む
#  別の箱を積む
#  違うもの(例えば大きさが異なる箱、ペットボトル)を掴んでみる
#    対象物位置は既知とする
#    アプローチ、把持姿勢の計算
#    ハンドの把持角度 or 指先間隔の設定
#    実機での動作実験 (机に当たらないように注意)
#  箱を並べる(押す、滑らせる)
#  ハノイの塔をやってみる


# 動作計画(仕様が変わる可能性大)
traj = test_plan() # RRT-connectによる動作計画
show_traj(traj) # 軌道の表示
opttraj = pl.optimize_trajectory(traj) # 軌道の最適化
show_traj(opttraj)
opttraj = pl.optimize_trajectory(opttraj) # さらにスムージング

# 干渉チェックにはPQPを利用
# ロボットのモデルVRMLそのまま
# 環境はbox, AABBのみ (AABB=>triangular mesh=>PQP)

# 干渉チェック対象物の追加例
r.add_collision_object(env.get_object('table top'))


# calibration（ROSのツールを使う. wilikiのROSEUSを参照）
# 単眼(右手の場合)
r.set_joint_angles(motions.calib_pose)
sync()

$ rosrun camera_calibration cameracalibrator.py --size 7x5 --square 0.030 image:=/hiro/rhand/usb_cam/image_raw

# ステレオ
$ rosrun camera_calibration cameracalibrator.py --size 7x5 --square 0.030 right:=/hiro/reye/image_raw left:=/hiro/leye/image_raw right_camera:=/hiro/reye left_camera:/hiro/leye
# 左右カメラが同期していないとウィンドウが出てこない
#--approximate=0.010 オプションをつけると0.010秒までのタイムスタンプのずれを許容する
# 今の複数プロセスのキャプチャではこれでも足りない
# approximateを大きくすると顕著にcalib精度が低下する
# キャプチャの方法を変更する必要がある


# 双腕の利用、持ち替え
# テーブルにぴったり置きたい
# カゴにきれいに並べたい

# 便利関数
# effrm位置に手先（エンドエフェクタ位置と姿勢）が行くように腕を動かす
# arm='left'で左腕
# use_waist=Trueで腰軸を使う

# 滑らかな軌道
# 現在、SequencePlayerで滑らかに軌道をつなぐことができない
# 直前の動作終了前にsetJointAnglesで次の目標値を送ると
# 新しい目標値が無視される
# 110219のHIRO-NXバージョンではどうなっているか？

# 実装中
# 物体間の相対座標の変更
# シーングラフの表示

# 余裕があれば...
# 柔軟物をつかんでみる
# 適当な画像処理(賢人ブロック) (認識器を書く)

# 余談
# ミドルウェアについて
# ROS / RT-middleware
# 数値計算ライブラリ： numpy, scipy, plot

# 内部構造
# 要するに座標系の関係を操作しているに過ぎない
# jointにリンクがくっついたモデル（from 梶田本）
# IK: rosen IK, 解の選択
